GROUP 01

Members:
** Florian Chlan     fchlan@student.ethz.ch
** Sam Kessler       sakessle@student.ethz.ch
** Jovan Nikolic     jovan.nikolic@gess.ethz.ch
** Jovan Andonov     andonovj@student.ethz.ch

========================================================================
TRAINING PARAMETERS USED:

We trained each model for 10 epochs (sweeps of data), that on Tesla M60 GPU available on Azure, took approximately 7 to 8 hours. The results reported are obtained after training the model for 2 epochs, as they were the most representative and generalized the best on the test set. To reproduce, our model checkpoints can be downloaded from:

https://polybox.ethz.ch/index.php/s/JHA4guMyYq9OEU9

Attached, you will find the following results:

group01.perplexityA
group01.perplexityB
group01.perplexityC
group01.continuation

We have used up all funds available on Azure during testing and training our models.

========================================================================
RUNNING THE CODE:

Requirements:
  - We expect all data (sentences.train, sentences.test, sentences.continuation) to be in ./data folder, 
  where current folder is folder in which *.py scripts are

  - We expect wordembeddings-dim100.word2vec file in the same directory as *.py scripts

------------------------------------------------------------------------

To train model for experiment X (A, B or C), use

python3 main.py -x X 

This will:
** preprocess training data (splitting sentences in words; adding <bos>, <eos>, <unk> and <pad> tokens; removing sentences longer than 28 words)
** serialize and write to disc the following files:
	- padded_sentences.pickle
	- vocabulary.pickle
	- word_2_index.pickle
	- index_2_word.pickle
   On next training, the script will use existing pickle files. The files are saved in the same directory as *.py scripts.
** save trained graph at given frequency. By default, graph is saved in the same directory as *.py scripts. The name of the graph is of the following format:
	expX-epY-NUM.*
   where X is substituted with A, B or C, Y is the epoch and NUM is number of batches/steps the model is trained on. Note that after training on one sweep of data, number of batches is not reset, but it's continiously incrementing.

------------------------------------------------------------------------ 

Perplexity calculations for experiment X are obtained by running:

python3 perplexity.py -x X -c <path_to_checkpoint>

where:
	- X can be substituted with A, B or C for each experiment
	- <path_to_checkpoint> is path to the trained graph. By default, graphs are stored in the same directory as *.py scripts.

Requirements:
	- word_2_index.pickle and index_2_word.pickle in the same directory as *.py scripts.

This will output the file named "group01.perplexityX", where X is substituted with A, B or C.

------------------------------------------------------------------------

Continuation of sentences is generated by running:

python3 continuation.py -x C -c <path_to_checkpoint>

where:
	- <path_to_checkpoint> is path to the trained graph. By default, graphs are stored in the same directory as *.py scripts.

Requirements:
	- word_2_index.pickle and index_2_word.pickle in the same directory as *.py scripts.

This will output file "group01.continuation".
Note that continuation is using model trained in experiment C. 

========================================================================
